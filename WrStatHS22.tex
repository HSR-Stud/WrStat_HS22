% !TeX program = xelatex
% !TeX encoding = utf8
% !TeX root = WrStatHS22.tex

%% TODO: publish to CTAN
\documentclass[margin=normal]{tex/hsrzf}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Packages

%% TODO: publish to CTAN
\usepackage{tex/hsrstud}

%% Language configuration
\usepackage{multicol}
\usepackage{polyglossia}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\pgfplotsset{compat = newest}
\setdefaultlanguage[variant=swiss]{german}

%% License configuration
\usepackage[
    type={CC},
    modifier={by-nc-sa},
    version={4.0},
    lang={german},
]{doclicense}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Metadata

\course{Elektrotechnik}
\module{WrStat}
\semester{Herbstsemester 2022}

\authoremail{joel.leirer@ost.ch}
\author{\textsl{Joël Leirer} -- \texttt{\theauthoremail}}

% did someone help you with this work?
\contributors{

}

\title{\texttt{\themodule} Zusammenfassung}
\date{\thesemester}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Document

\begin{document}

% use roman numberals for introductiory pages
\pagenumbering{roman}

\maketitle

% \begin{abstract}
% \end{abstract}

% show the names of the people who contributed to this document.
% \section*{Contributors}
% \thecontributors


\section*{Lizenz}
\doclicenseThis

\clearpage

\tableofcontents

% actual content
\clearpage
\setcounter{page}{1}
\pagenumbering{arabic}

\section{Kombinatorik}
\subsection{Zählregeln}
\begin{tabular}{c c c}
    Disjunktive Vereinigung                & Schnittmengen & Produkt \\
    \begin{tikzpicture}
        \fill[red!30!white]   (0,0) circle (0.8);
        \fill[green!30!white] (1.7,0) circle (0.8);
        \node at (0,0)    {$A$};
        \node at (1.7,0)   {$B$};
    \end{tikzpicture}
                                           &
    \begin{tikzpicture}
        \begin{scope}[blend group = soft light]
            \fill[red!30!white]   ( 90:0.8) circle (0.8);
            \fill[green!30!white] (180:0.8) circle (0.8);
        \end{scope}
        \node at ( 90:0.8)  {$A$};
        \node at (180:0.8)   {$B$};
    \end{tikzpicture}
                                           &
    \begin{tikzpicture}
        \fill[black!30!white] (0.3,-0.3) rectangle (1.6,-1.6);
        \fill[red!30!white]  (0.3,0) rectangle (1.6,-0.25);
        \fill[green!30!white](0,-0.3) rectangle (0.25,-1.6);
        \node at (0.95, 0.2) {$A$};
        \node at (-0.2, -0.95) {$B$};
        \node at (0.95, -0.95) {$ A \times B $};
    \end{tikzpicture}
    \\\\
    $|A \cup B| = |A| + |B| $              &
    $|A \cup B| = |A| + |B| - |A \cap B| $ &
    $|A \times B| = |A| \cdot |B| $
\end{tabular}
\subsection{Kombinatorik - Regeln}
\begin{tabular}{|l| p{10cm}|}
    \hline Anordnung - Permutation &
    auf wie viele Arten kann man $n$ Objekte anordnen? \newline
    $ n \cdot (n-1) \cdot (n-2) \cdots \cdot 1 = n! $         \\
    \hline Auswahl - Kombination   &
    Auf wie viele Arten kann man k aus n Auswählen? \newline
    Auswahl: $ n \cdot (n-1) \cdot \cdots \cdot (n-(k+1)) = \frac{n!}{(n-k)!}$
    Anordnung der Auswahl:  $k \cdots (k-1) \cdot \cdots \cdot = k!$
    Insgesamt: $ C^n_k = \frac{n!}{k!(n-k)!} = \binom{n}{k} $ \\
    \hline Variation               &
    Perlenkettenproblem:
    auf wie viele Arten kann man eine Perlenkette der Länge k aus n Farben herstellen? \newline
    $V_n,k = n \cdot n^k-1 = n^k$                             \\
    \hline Erzeugende Funktion     &
    $f(z) = \sum \limits _{n = 0} ^{\infty} a_n \cdot z^n
        = a_0 + a_1 \cdot z + a_2 \cdot z^2 + \dots + a_n \cdot z^n$
    \\
    \hline
\end{tabular}

\section{Ereignisse und Wahrscheinlichkeit}
\subsection{Ereignisse}
\begin{multicols}{2}
    \begin{tikzpicture}
        \begin{scope}[blend group = soft light]
            \fill [red!30!white] (0,0) circle [x radius = 1.2, y radius = 1.8];
            \fill [green!30!white] (2,-1) circle [y radius = 1.2, x radius = 1.8];
        \end{scope}
        \node [red] at (0.0, 1) {$A$};
        \node [green] at (3, -0.95) {$B$};
        \node [black]at (0, 0.2) {$ \omega $};
        \node [black]at  (3.5, 1.5) {$ \Omega $};
        \fill [black] (0,0) circle [radius = 0.05];
        \draw [black] (-1.5,2) rectangle (4,-2.5);
    \end{tikzpicture}
    \begin{itemize}
        \item $\omega$ = Elementarereignis (Versuchsausgang)
        \item $\Omega$ = Menge der Elementarereignisse
        \item $A$ und $B$ = Teilmengen von $\Omega$ (hier $\omega \in A, \omega \notin B$)
        \item Spezialfälle:
              \begin{itemize}
                  \item $A =\Omega \subset \Omega$: A tritt immer ein (sicheres Ereignis)
                  \item $B = \emptyset \subset \Omega$: B tritt nie ein (unmögliches Ereignis)
              \end{itemize}
    \end{itemize}
\end{multicols}
\begin{tabular}{l m{4cm}}
    \begin{tabular}{|p{2.5cm} |l|}
        \hline
        Modell                          & Begriff                                      \\
        \hline
        $\omega$                        & Versuchsausgang, Elementarereignis           \\
        $\Omega$                        & alle Versuchsausgänge                        \\
        $A \subset \Omega$              & Ereignis                                     \\
        $\omega \in A$                  & Ereignis ist eingetreten                     \\
        $\Omega$                        & sicheres Ereignis, tritt immer ein           \\
        $\emptyset$                     & unmögliches Ereignis, kann nicht eingetreten \\
        $A \cap B$                      & A und B treten ein                           \\
        $A \cup B$                      & A oder B treten ein                          \\
        $A \subset B$                   & A hat B zur Folge                            \\
        $\bar{A} = \Omega \setminus A $ & nicht A                                      \\
        \hline
    \end{tabular}
     &
    Rechenregeln:
    \begin{itemize}
        \item $A \cap ( B \cup C) = (A \cap B ) \cup (A \cap C) $
        \item $A \cup ( B \cap C) = (A \cup B ) \cap (A \cup C) $
        \item $\overline{A \cap B} = \overline{A}  \cup \overline{B}$
        \item  $\overline{A \cup B} = \overline{A} \cap \overline{B}$
    \end{itemize}
\end{tabular}
\subsection{Wahrscheinlichkeit}
\begin{tabular}{m{7.5cm} m{12cm}}
    \subsubsection*{Axiome eines Wahrscheinlichkeitsraumes}
    \begin{itemize}
        \item Wertebereich: \newline $0 \leq P(A) \leq 1$
        \item Wahrscheinlichkeit sicheres Ereignis: \newline$P(\Omega) = 1$
        \item Disjunktive Vereinigung:\newline $P(A_1 \cup A_2 \cup \dots \cup A_n \cup \dots)
                  \newline = P(A_1) + P(A_2) + \dots + P(A_n) + \dots$
    \end{itemize}
     &
    \subsubsection*{Eigenschaften der Wahrscheinlichkeit}


    \begin{itemize}
        \item Wahrscheinlichkeit des unmöglichen Ereignisses \newline $P(\emptyset) = 0$ \newline
              Auch $\neq \emptyset$ Ereignisse können Wahrscheinlichkeit 0 haben!
        \item Wahrscheinlichkeit des komplementären Ereignisses \newline
              $P(\overline{A}) = P(\Omega \setminus A) = 1 - P(A)$
        \item Wahrscheinlichkeit der Differenz zweier Ereignisse \newline
              $P(A \setminus B) = P(A) - P(A \cap B)$
        \item Wahrscheinlichkeit der Vereinigung zweier Ereignisse \newline
              $P(A \cup B) = P(A) + P(B) - P(A \cap B)$
    \end{itemize}
\end{tabular}
\begin{multicols}{2}

    \subsubsection{Bedingte Wahrscheinlichkeit}
    Wahrscheinlichkeit das Ereignis $P(A)$ eintrifft,
    wenn Ereignis $P(B)$ eingetroffen ist: $P(A|B)$
    $$P(A|B)\frac{P(A\cap B)}{P(B)}$$
    wenn A und B unabhängig:
    $$P(A \cap B) = P(A) \cdot P(B)$$
    \subsubsection*{Satz von Bayes ("Bedingungsumkehrung")}
    $$P(R|T) \cdot P(T) = P(R \cap T) = P(T|R) \cdot P(R)$$
    $$P(R|T) = P(T|R) \frac{P(R)}{P(T)}\;
        P(T|R) = P(R|T) \frac{P(T)}{P(R)}$$
    \subsubsection{Totale Wahrscheinlichkeit}
    Sind nur Bedingte Wahrscheinlichkeiten $P(A|B)$ und die bedingende Wahrscheinlichkeit
    $P(B)$ bekannt, ergibt sich die totale Wahrscheinlichkeit $P(A)$ aus:
    $$P(A) = P(A|B) \cdot P(B) + P(A|\bar{B})\cdot P(\bar{B})$$
    \subsection*{Note}
    \begin{itemize}
        \item Laplace-Experiment: Experiment bei dem jedes Elementarereignis gleich wahrscheinlich ist.
        \item Bernoulli-Experiment: Experiment mit zwei Versuchsausgängen mit Wahrscheinlichkeiten p und 1-p.
    \end{itemize}
\end{multicols}

\section{Statistik}
\begin{multicols}{2}

    \subsection{Begriffe}
    \subsubsection*{Arithmetischer Mittelwert:}
    $$\bar{x} = \frac{x_0 + \dots + x_1}{n} = \frac{1}{n} \sum_{i=1}^n x_i $$
    \subsubsection*{Gewichteter Mittelwert:}
    $$\bar{x} = \frac{w_1x_1 + w_2x_2 + \dots + w_nx_n}{w_1 + w_2 + \dots + w_n}
        = \frac{\sum _{k=1}^n W_kx_k}{\sum_{k=1}^n w_k}$$
    Eigenschaften:
    $$y_i = ax_i + b \Rightarrow \bar{y} = a\bar{x} + b$$

    \subsubsection*{Geometrisches Mittel}
    $$G= \sqrt[n]{x_1\cdot x_2\cdot\dots\cdot x_n} = \sqrt[n]{\prod x_i}$$

    \subsubsection*{Median}
    Median ist ein Wert einer Liste der in der Mitte steht wenn sie der Grösse nach sortiert wird.
    Bei einer geraden Anzahl Werte ist er das Arithmetische Mittel der beiden mittigsten Werte.
    Bsp: $x = {74, 4, 8, 2, 1355, 6643} \Rightarrow x_{0.5} = 41$ \\
    Eigenschaften:
    $$y_i = ax_i + b \Rightarrow med(y_i) = a \cdot med(x_i) + b$$
    $$ med(x_i) = min(\sum _{i=1} ^n |x_i - \bar{x}|)$$

    \subsubsection*{Quantil}
    Eine Unterteilung der Beobachtungswerte bzw. der Fläche der Dichtekurve.
    $p$-Quantil $p \in \mathbb{R}$ links davon ist der Anteil $p$ in \% aller Zufallswerte bzw. Der Dichtekurve.



    \subsection{diskrete Zufallsvariable}

    Eine Zufallsvariable X ist eine Funktion die einem
    Versuchsausgang $\omega$ einen Wert $X(\omega)$ zuordnet.
    Sie ist \textbf{diskret} wenn sie einzelne genaue Zahlenwerte annehmen kann.
    Wenn sie beliebige Werte annehmen kann ist sie \textbf{stetig}.

    \subsubsection*{Erwartungswert}
    Der Erwartungswert ist der Wert der Zufallsvariable multipliziert mit ihrer Wahrscheinlichkeit.
    $$E(x) = \sum Wert \cdot Wahrscheinlichkeit$$
    $$ =\sum _{\omega \in \Omega} X(\omega) \cdot P({\omega}) = \sum _{i=1} ^{k} X(A_i) \cdot P(A_i)$$
    \subsubsection*{Empirischer Erwartungswert}

    $$ E(X) = \sum _i X(i) \cdot P(X=x_i) \approx \sum _i x_i \frac{n_i}{n} = \frac{x_1n_1 + x_2n_2 + \dots + x_kn_k}{n}$$

    \subsubsection*{Rechenregeln}
    $$E(X+Y) = E(X) + E(Y)$$
    $$E(\lambda X) = \lambda E(X)$$

    Vorzeichen von XY unkorreliert $\Rightarrow E(XY) = E(X)E(Y)$
    Unabhängig: $XY \geq 0 \Rightarrow E(XY) \geq 0$ oder $XY\leq 0 \Rightarrow E(XY) \leq 0$

    X und Y sind unabhängig, wenn die Ereignisse x und y unabhängig sind.

    \subsection{Varianz - Streumass}
    Die Varianz ist die Erwartete Abweichung vom Erwartungswert.
    Sie misst die Streuung einer Zufallsvariable.

    $$var(x) = E((|X-E(X)|)^2) = E(X^2) - E(X)^2$$

    wenn X und Y \textbf{unabhängig} (oder unkorreliert):
    $$var(\lambda X) = \lambda^2 var(x)$$
    $$var(X+Y) = var(X) + var(Y)$$

    \subsubsection*{Satz von Tschebyscheff}
    Wenn X eine Zufallsvariable mir Erwartungswert $\mu = E(X)$ ist, dann gilt:
    $$P(|X-\mu|)\leq \frac{var(X)}{\epsilon^2}$$
    für $X-\mu = A$ und $X-\mu > \epsilon$

    \subsubsection*{Satz von Bernoulli}
    \textbf{Stichprobe:} $X_1 \dots X_n$ sind Zufallsvariablen mit gleicher Varianz und und gleichem Erwartungswert wie X.
    \\ \textbf{Mittelwert:} $M_n = \frac{X_1+\dots+ X_n}{n}$, $E(M_n) = E(X), var(M_n) =\frac{1}{n} var(X)$
    \\ \textbf{Satz:} $$P(|M_n -\mu|) \leq \frac{var(X)}{n\epsilon^2} $$
    für $|M_n -\mu| > \epsilon$
    \subsubsection*{Gesetz grosser Zahlen}
    Angenommen die Wahrscheinlichkeit von Ereigniss A ist gleich dem Erwartungswert ($P(A) = E(X)$), dann gilt:
    \\ \textbf{relative Häufigkeit:} $h_n = \frac{X_1+ \dots + X_n}{n}$, $E(h_n) = P(A)$ , $ var(h_n) = \frac{1}{n} var(X) = \frac{1}{n}P(A)(1-P(A))$
    \\ \textbf{Satz von Bernoulli:}
    $$P(|h_n - P(A)|) \leq \frac{P(A)(1-P(A))}{n\epsilon^2} \leq \frac{1}{4n\epsilon^2} $$
    für $|h_n - P(A)| > \epsilon$

    \tiny{Je grösser die Anzahl Durchführungen, desto kleine die Wahrscheinlichkeit, dass die relative Häufigkeit stark von der Wahrscheinlichkeit abweicht.}
\end{multicols}


\subsection*{Lineare Regression}
\begin{multicols}{2}

    \begin{tikzpicture}
        \begin{axis}[
                xmin = 0, xmax = 11,
                ymin = 0, ymax = 11,
                width = 0.45\textwidth,
                height = 0.35\textwidth,
                xtick distance = 1,
                ytick distance = 1,
                grid = both,
                minor tick num = 1,
                major grid style = {lightgray},
                minor grid style = {lightgray!25},
                xlabel = {Time ($t$)},
                ylabel = {Position ($x$)},
                legend cell align = {left},
                legend pos = north west
            ]
            % Plot data
            \addplot[
                teal,
                only marks
            ] table[x = t, y = x] {LinearReg.dat};
            % Linear regression
            \addplot[
                thick,
                orange
            ] table[
                    x = t,
                    y = {create col/linear regression={y=x}}
                ] {LinearReg.dat};
            % Add legend
            \addlegendentry{Daten}
            \addlegendentry{Lineare Regression};
        \end{axis}
    \end{tikzpicture}

    Linerer Zusammenhang zwischen Zufallsvariablen X und Y:
    $$ Y = aX + b + Fehler$$
    wobei, a und b zu Bestimmen sind, damit der Fehler möglichst klein wird.
    \\ \textbf{Gleichungssystem:}
    $$ E(X^2)a + E(X)b = E(XY) und E(X)a + b = E(Y)$$
    daraus folgt:
    $$a= \frac{E(XY)- E(X)E(Y)}{E(X^2) - E(X)^2} = \frac{cov(X,Y)}{var(X)}$$
    $$b= E(Y) - aE(X)$$

    \textbf{Covarianz}
    $$cov(X,Y) = E((X-E(X))\cdot(Y-E(Y)))$$
    $$cov(X,Y) = E(XY) - E(X)E(Y)$$

    \textbf{Fehlerberechnung:}
    $$var(Y-aX-b) = var(Y)(1-\frac{cov(X,Y)^2}{var(X)var(Y)}) $$
    $$\frac{var(Y-aX-b)}{var(Y)} = 1- \frac{cov(X,Y)^2}{var(X)var(Y)} = 1 -r^2 $$
    \textbf{Regressionskoeffizient:}
    $$r = \frac{cov(X,Y)}{\sqrt{var(X) \cdot var(Y)}} = \frac{E(XY) - E(X)E(Y)}{\sqrt{var(X) \cdot var(Y)}}$$
    wenn $r^2 \simeq  1$ Passt das Modell gut.
\end{multicols}

\subsection{Stetige Zufallsvariable}
\begin{multicols}{2}
    
    \subsubsection*{Verteilungsfunktion}
    Die Verteilungsfunktion beschreib die Wahrscheinlichkeiten der Werte einer Zufallsvariable.
$$F(x)= F_X(x) = P(X \leq x)$$
Eigenschaften:
\begin{itemize}
    \item monoton wachsend
    \item $0 \leq F_X(x) \leq 1$
    \item $\lim_{x \to -\infty}  F(x) = 0$
    \item $lim_{x \to \infty} F(x) = 1$
    \item $P(a < X \leq b) = F_x(b) - F_X(a)$
\end{itemize}

\subsubsection*{Wahrscheinlichkeitsdichte:}
Die Wahrscheinlichkeitsdichte ist die Ableitung der Verteilungsfunktion.
Ihre Fläche \textbf{muss} 1 sein.

$$\varphi(x) = \varphi_X(x) = F'_X(x)$$
$$F_X(x) = \int \limits _{-\infty} ^{x} \varphi(\xi)d\xi$$

Eigenschaften:
\begin{itemize}
    \item $\varphi(x) \geq 0$
    \item $\int \limits _{-\infty} ^{\infty} \varphi(x)dx = 1$
    \item $F_X(x)$ ist die Stammfunktion.
    \item $P(a < X \leq b) =\int \limits _a ^b \varphi_X(x)dx$
\end{itemize}

\subsubsection{Erwartungswert:}
$$E(X) = \sum Wert \cdot Wahrscheinlichkeit = \int \limits _{-\infty} ^{\infty} x \cdot \varphi(x)dx $$
$$E(X^2) = \int \limits _{-\infty} ^{\infty} x^2 \cdot \varphi(x) dx$$
$$E(f(X)) = \int \limits _{-\infty} ^{\infty} f(x) \cdot \varphi(x)dx$$

\subsubsection*{Summe Stetiger Zufallsvariablen}
wemm $X$ und $Y$ unabhängig sind, dann hat die Zufallsvariable $Z = X + Y$ die Wahrscheinlichkeitsdichte:
$$\varphi_Z(z) = \varphi_X * \varphi_Y(z) = \int \limits _{-\infty} ^{\infty}  \varphi_X(x) \varphi_Y(z-x)dx$$

\subsubsection*{Summe von Gleichverteilungen}
$$\varphi_X(x) = \varphi_Y(x)$$
$$\varphi_{X+Y}(x) = \varphi_X * \varphi_Y(x) = \int \limits _{-\infty} ^{\infty} \varphi(y)\cdot \varphi(x-y)dy $$
\end{multicols}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Anhang
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Anhang}

\subsection{Taschenrechner}

\newpage
\subsection{Berechnungstabelle Lineare Regression}

\begin{tabular}{|p{0.5cm}|p{1cm} p{2cm}|p{2cm} p{3cm}| p{4cm}|}
    \hline
    $i$   & $t_i$ & $t_i^2$ & $x_i$ & $x_i^2$ & $t_ix_i$ \\
    \hline
    \hline
    1     &       &         &       &         &          \\[20pt]
    \hline
    2     &       &         &       &         &          \\[20pt]
    \hline
    3     &       &         &       &         &          \\[20pt]
    \hline
    4     &       &         &       &         &          \\[20pt]
    \hline
    5     &       &         &       &         &          \\[20pt]
    \hline
    6     &       &         &       &         &          \\[20pt]
    \hline
    7     &       &         &       &         &          \\[20pt]
    \hline
    8     &       &         &       &         &          \\[20pt]
    \hline
    9     &       &         &       &         &          \\[20pt]
    \hline
    10    &       &         &       &         &          \\[20pt]
    \hline
    11    &       &         &       &         &          \\[20pt]
    \hline
    12    &       &         &       &         &          \\[20pt]
    \hline
    13    &       &         &       &         &          \\[20pt]
    \hline
    14    &       &         &       &         &          \\[20pt]
    \hline
    15    &       &         &       &         &          \\[20pt]
    \hline
    16    &       &         &       &         &          \\[20pt]
    \hline
    17    &       &         &       &         &          \\[20pt]
    \hline
    18    &       &         &       &         &          \\[20pt]
    \hline
    19    &       &         &       &         &          \\[20pt]
    \hline
    20    &       &         &       &         &          \\[20pt]
    \hline
          &       &         &       &         &          \\
    Total &       &         &       &         &          \\
    \hline
    \hline
\end{tabular}

\end{document}
